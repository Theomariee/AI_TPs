{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5 & 6 - Artifical Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marlon KUQI & ThÃ©o MARIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Understanding convolutional networks for computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.2646 - acc: 0.9185 - val_loss: 0.0549 - val_acc: 0.9816\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0898 - acc: 0.9739 - val_loss: 0.0369 - val_acc: 0.9876\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0661 - acc: 0.9804 - val_loss: 0.0369 - val_acc: 0.9874\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0555 - acc: 0.9831 - val_loss: 0.0311 - val_acc: 0.9890\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.0479 - acc: 0.9859 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.0343 - val_acc: 0.9881\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0397 - acc: 0.9883 - val_loss: 0.0283 - val_acc: 0.9898\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0365 - acc: 0.9895 - val_loss: 0.0267 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0313 - acc: 0.9902 - val_loss: 0.0258 - val_acc: 0.9909\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0284 - val_acc: 0.9900\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0280 - acc: 0.9917 - val_loss: 0.0246 - val_acc: 0.9914\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.0271 - acc: 0.9914 - val_loss: 0.0257 - val_acc: 0.9913\n",
      "Test loss: 0.02571112294295017\n",
      "Test accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# \"filters\" -> the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# \"kernel_size\" -> An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
    "#                  Can be a single integer to specify the same value for all spatial dimensions.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Down sampling with MaxPooling2D\n",
    "# \"pool_size\" -> integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). \n",
    "#                (2, 2) will halve the input in both spatial dimension. \n",
    "#                If only one integer is specified, the same window length will be used for both dimensions.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Training a convnet on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 56s 451ms/step - loss: 0.6974 - acc: 0.5520 - val_loss: 0.6526 - val_acc: 0.6238\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 56s 448ms/step - loss: 0.6366 - acc: 0.6235 - val_loss: 0.6865 - val_acc: 0.5875\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 57s 453ms/step - loss: 0.6179 - acc: 0.6930 - val_loss: 0.5811 - val_acc: 0.7050\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 59s 473ms/step - loss: 0.5888 - acc: 0.7005 - val_loss: 0.5817 - val_acc: 0.6950\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 57s 456ms/step - loss: 0.5466 - acc: 0.7325 - val_loss: 0.5441 - val_acc: 0.7362\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 57s 458ms/step - loss: 0.5225 - acc: 0.7500 - val_loss: 0.5236 - val_acc: 0.7612\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 63s 500ms/step - loss: 0.4985 - acc: 0.7600 - val_loss: 0.5682 - val_acc: 0.7175\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 64s 513ms/step - loss: 0.4803 - acc: 0.7835 - val_loss: 0.5750 - val_acc: 0.7262\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.4729 - acc: 0.7885 - val_loss: 0.5523 - val_acc: 0.7238\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.4521 - acc: 0.8000 - val_loss: 0.5452 - val_acc: 0.7350\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.4308 - acc: 0.8060 - val_loss: 0.5394 - val_acc: 0.7538\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.4212 - acc: 0.8170 - val_loss: 0.5348 - val_acc: 0.7600\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 50s 397ms/step - loss: 0.3930 - acc: 0.8300 - val_loss: 0.5537 - val_acc: 0.7688\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 49s 391ms/step - loss: 0.4066 - acc: 0.8205 - val_loss: 0.6268 - val_acc: 0.7275\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 49s 390ms/step - loss: 0.3690 - acc: 0.8405 - val_loss: 0.5707 - val_acc: 0.7475\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3653 - acc: 0.8440 - val_loss: 0.5915 - val_acc: 0.7375\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 49s 388ms/step - loss: 0.3674 - acc: 0.8410 - val_loss: 0.5572 - val_acc: 0.7488\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 49s 388ms/step - loss: 0.3549 - acc: 0.8390 - val_loss: 0.5809 - val_acc: 0.7488\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.3744 - acc: 0.8425 - val_loss: 0.5952 - val_acc: 0.7388\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 50s 398ms/step - loss: 0.3348 - acc: 0.8520 - val_loss: 0.7850 - val_acc: 0.7512\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 50s 398ms/step - loss: 0.3777 - acc: 0.8345 - val_loss: 0.6307 - val_acc: 0.7512\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 49s 391ms/step - loss: 0.3356 - acc: 0.8620 - val_loss: 0.6450 - val_acc: 0.7438\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3478 - acc: 0.8550 - val_loss: 0.6252 - val_acc: 0.7600\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 49s 392ms/step - loss: 0.3337 - acc: 0.8575 - val_loss: 0.5915 - val_acc: 0.7488\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 50s 397ms/step - loss: 0.3169 - acc: 0.8770 - val_loss: 0.5396 - val_acc: 0.7312\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 49s 390ms/step - loss: 0.3125 - acc: 0.8765 - val_loss: 0.6166 - val_acc: 0.7338\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.3225 - acc: 0.8655 - val_loss: 1.5251 - val_acc: 0.6713\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 51s 407ms/step - loss: 0.3404 - acc: 0.8645 - val_loss: 0.8039 - val_acc: 0.7538\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 49s 392ms/step - loss: 0.3165 - acc: 0.8730 - val_loss: 0.6351 - val_acc: 0.7400\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 49s 391ms/step - loss: 0.3109 - acc: 0.8720 - val_loss: 0.8601 - val_acc: 0.7375\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 50s 396ms/step - loss: 0.3113 - acc: 0.8770 - val_loss: 0.7569 - val_acc: 0.7512\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 48s 386ms/step - loss: 0.3178 - acc: 0.8820 - val_loss: 0.6992 - val_acc: 0.7638\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3490 - acc: 0.8625 - val_loss: 0.9376 - val_acc: 0.7562\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3237 - acc: 0.8610 - val_loss: 0.6843 - val_acc: 0.7612\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 49s 390ms/step - loss: 0.3215 - acc: 0.8715 - val_loss: 0.7614 - val_acc: 0.7700\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 49s 391ms/step - loss: 0.3486 - acc: 0.8765 - val_loss: 0.6066 - val_acc: 0.7488\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 49s 388ms/step - loss: 0.3206 - acc: 0.8695 - val_loss: 0.7474 - val_acc: 0.7450\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3328 - acc: 0.8835 - val_loss: 0.6661 - val_acc: 0.7512\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3448 - acc: 0.8650 - val_loss: 0.7987 - val_acc: 0.7500\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 48s 387ms/step - loss: 0.3588 - acc: 0.8575 - val_loss: 0.5656 - val_acc: 0.7475\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 51s 407ms/step - loss: 0.3305 - acc: 0.8775 - val_loss: 0.6737 - val_acc: 0.7375\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 0.3655 - acc: 0.8660 - val_loss: 1.0745 - val_acc: 0.7562\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 54s 436ms/step - loss: 0.3794 - acc: 0.8600 - val_loss: 0.6072 - val_acc: 0.7375\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.4201 - acc: 0.8535 - val_loss: 0.6877 - val_acc: 0.7388\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 55s 442ms/step - loss: 0.4007 - acc: 0.8425 - val_loss: 0.6822 - val_acc: 0.7550\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 54s 429ms/step - loss: 0.3743 - acc: 0.8555 - val_loss: 0.5871 - val_acc: 0.7163\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 54s 429ms/step - loss: 0.3627 - acc: 0.8620 - val_loss: 0.7439 - val_acc: 0.7688\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 54s 430ms/step - loss: 0.3719 - acc: 0.8540 - val_loss: 0.7195 - val_acc: 0.7325\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 0.3536 - acc: 0.8635 - val_loss: 0.7056 - val_acc: 0.7625\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 50s 397ms/step - loss: 0.3750 - acc: 0.8505 - val_loss: 0.8784 - val_acc: 0.7700\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "'''\n",
    "    Training set :\n",
    "        Dogs : dogs.0 to dogs.599\n",
    "        Cats : cats.0 to cats.599\n",
    "        \n",
    "    Test set : \n",
    "        Dogs : dogs.600 to dogs.999\n",
    "        Cats : cats.600 to cats.999\n",
    "        \n",
    "    600 pictures of each category to train our model on\n",
    "    400 pictures of each category for our model to be tested\n",
    "    \n",
    "    Ratio : 66%\n",
    "'''\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/test'\n",
    "\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
