{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 & 4 - Artifical Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marlon KUQI & ThÃ©o MARIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Recurrent neural network / LSTM : IMDB sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.4633 - acc: 0.7804 - val_loss: 0.3843 - val_acc: 0.8317\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.3065 - acc: 0.8734 - val_loss: 0.3762 - val_acc: 0.8314\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.2212 - acc: 0.9126 - val_loss: 0.4469 - val_acc: 0.8295\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.1552 - acc: 0.9418 - val_loss: 0.4580 - val_acc: 0.8322\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.1054 - acc: 0.9619 - val_loss: 0.5490 - val_acc: 0.8184\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 77s 3ms/step - loss: 0.0841 - acc: 0.9698 - val_loss: 0.6345 - val_acc: 0.8225\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.0556 - acc: 0.9812 - val_loss: 0.8111 - val_acc: 0.8136\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0435 - acc: 0.9856 - val_loss: 0.7511 - val_acc: 0.8176\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.0340 - acc: 0.9885 - val_loss: 0.9008 - val_acc: 0.8153\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 75s 3ms/step - loss: 0.0241 - acc: 0.9922 - val_loss: 0.9831 - val_acc: 0.8134\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0156 - acc: 0.9951 - val_loss: 1.0041 - val_acc: 0.8135\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0170 - acc: 0.9942 - val_loss: 1.2020 - val_acc: 0.8119\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0149 - acc: 0.9952 - val_loss: 1.1054 - val_acc: 0.8070\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 74s 3ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.9712 - val_acc: 0.8040\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 1.1004 - val_acc: 0.8094\n",
      "25000/25000 [==============================] - 11s 433us/step\n",
      "Test score: 1.1003549643659591\n",
      "Test accuracy: 0.8094\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "# ---------- Data preparation -----------\n",
    "\n",
    "# Loads data from the imdb dataset\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "# Reshapes the training and testing sets\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# ---------- Model building -----------\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# The Embedding layer indicates that we work on a vocabulary (i.e. distinct words) of size \"max_features\"\n",
    "# Our embedding vectors will have a size of 128\n",
    "model.add(Embedding(max_features, 128))\n",
    "\n",
    "# The LSTM layer will have an output size of 128\n",
    "# \"dropout\" -> Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# \"recurrent_dropout\" -> Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# The last layer of our NN will be a regular one, of size 1 and activated by a sigmoid function\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
