{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 & 4 - Artifical Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marlon KUQI & ThÃ©o MARIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Recurrent neural network / LSTM : IMDB sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoders/Decoders for LTSM](https://qph.fs.quoracdn.net/main-qimg-febee5b881545802a75c064a84ecf85d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.4633 - acc: 0.7804 - val_loss: 0.3843 - val_acc: 0.8317\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.3065 - acc: 0.8734 - val_loss: 0.3762 - val_acc: 0.8314\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.2212 - acc: 0.9126 - val_loss: 0.4469 - val_acc: 0.8295\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.1552 - acc: 0.9418 - val_loss: 0.4580 - val_acc: 0.8322\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.1054 - acc: 0.9619 - val_loss: 0.5490 - val_acc: 0.8184\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 77s 3ms/step - loss: 0.0841 - acc: 0.9698 - val_loss: 0.6345 - val_acc: 0.8225\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.0556 - acc: 0.9812 - val_loss: 0.8111 - val_acc: 0.8136\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0435 - acc: 0.9856 - val_loss: 0.7511 - val_acc: 0.8176\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.0340 - acc: 0.9885 - val_loss: 0.9008 - val_acc: 0.8153\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 75s 3ms/step - loss: 0.0241 - acc: 0.9922 - val_loss: 0.9831 - val_acc: 0.8134\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0156 - acc: 0.9951 - val_loss: 1.0041 - val_acc: 0.8135\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0170 - acc: 0.9942 - val_loss: 1.2020 - val_acc: 0.8119\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 76s 3ms/step - loss: 0.0149 - acc: 0.9952 - val_loss: 1.1054 - val_acc: 0.8070\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 74s 3ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.9712 - val_acc: 0.8040\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 1.1004 - val_acc: 0.8094\n",
      "25000/25000 [==============================] - 11s 433us/step\n",
      "Test score: 1.1003549643659591\n",
      "Test accuracy: 0.8094\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "# ---------- Data preparation -----------\n",
    "\n",
    "# Loads data from the imdb dataset\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "# Reshapes the training and testing sets\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# ---------- Model building -----------\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# The Embedding layer indicates that we work on a vocabulary (i.e. distinct words) of size \"max_features\"\n",
    "# Our embedding vectors will have a size of 128\n",
    "model.add(Embedding(max_features, 128))\n",
    "\n",
    "# The LSTM layer will have an output size of 128\n",
    "# \"dropout\" -> Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# \"recurrent_dropout\" -> Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# The last layer of our NN will be a regular one, of size 1 and activated by a sigmoid function\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiles the model\n",
    "# \"loss\" -> Loss function used, here binary_crossentropy (i.e. LogLoss)\n",
    "# \"optimizer\" -> Optimizer for the convergence of the loss function, here adam. We used SGD in the last TPs\n",
    "# \"metrics\" -> List of metrics to be evaluated during testing and training\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our first results give the following figures:\n",
    "\n",
    "Test score: 1.1003549643659591\n",
    "\n",
    "Test accuracy: 0.8094"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Text classification: the Ohsumed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = 23\n",
    "current_category = \"03\"\n",
    "\n",
    "training_path = \"ohsumed-first-20000\\\\training\\C\"\n",
    "test_path = \"ohsumed-first-20000\\\\test\\C\" \n",
    "\n",
    "#tmp_file = pd.read_csv(\"ohsumed-first-20000\\training\\C01\")\n",
    "\n",
    "wordcount = defaultdict(int)\n",
    "\n",
    "for root, dirs, files in os.walk(training_path + current_category):  \n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        with open(training_path + current_category + \"\\\\\" + filename) as file:\n",
    "            for word in file.read().split():\n",
    "                wordcount[word] += 1\n",
    "\n",
    "    for k,v in wordcount.items():\n",
    "        print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ohsumed-first-20000-docs/</th>\n",
       "      <th>5</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>5.1</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>5.2</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>0</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>mentoplasty</th>\n",
       "      <th>using</th>\n",
       "      <th>Mersilene</th>\n",
       "      <th>mesh.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Many</td>\n",
       "      <td>different</td>\n",
       "      <td>materials</td>\n",
       "      <td>are</td>\n",
       "      <td>available</td>\n",
       "      <td>for</td>\n",
       "      <td>augmentation</td>\n",
       "      <td>mentoplasty.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>However,</td>\n",
       "      <td>the</td>\n",
       "      <td>optimal</td>\n",
       "      <td>implant</td>\n",
       "      <td>material</td>\n",
       "      <td>for</td>\n",
       "      <td>chin</td>\n",
       "      <td>implantation</td>\n",
       "      <td>has</td>\n",
       "      <td>yet</td>\n",
       "      <td>to</td>\n",
       "      <td>be</td>\n",
       "      <td>found.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The</td>\n",
       "      <td>material</td>\n",
       "      <td>provides</td>\n",
       "      <td>excellent</td>\n",
       "      <td>tensile</td>\n",
       "      <td>strength,</td>\n",
       "      <td>durability,</td>\n",
       "      <td>and</td>\n",
       "      <td>surgical</td>\n",
       "      <td>adaptability.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Based</td>\n",
       "      <td>on</td>\n",
       "      <td>this</td>\n",
       "      <td>10-year</td>\n",
       "      <td>experience,</td>\n",
       "      <td>Mersilene</td>\n",
       "      <td>mesh</td>\n",
       "      <td>remains</td>\n",
       "      <td>our</td>\n",
       "      <td>material</td>\n",
       "      <td>of</td>\n",
       "      <td>choice</td>\n",
       "      <td>for</td>\n",
       "      <td>chin</td>\n",
       "      <td>augmentation.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>intracranial</td>\n",
       "      <td>mucoceles</td>\n",
       "      <td>associated</td>\n",
       "      <td>with</td>\n",
       "      <td>phaeohyphomycosis</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>paranasal</td>\n",
       "      <td>sinuses.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ohsumed-first-20000-docs/         5 Unnamed: 2 Unnamed: 3           5.1  \\\n",
       "0                       NaN      Many  different  materials           are   \n",
       "1                       NaN  However,        the    optimal       implant   \n",
       "2                       NaN       The   material   provides     excellent   \n",
       "3                       NaN     Based         on       this       10-year   \n",
       "4                       NaN         0        NaN        NaN  intracranial   \n",
       "\n",
       "    Unnamed: 5  Unnamed: 6           5.2         Unnamed: 8 Unnamed: 9  \\\n",
       "0    available         for  augmentation       mentoplasty.        NaN   \n",
       "1     material         for          chin       implantation        has   \n",
       "2      tensile   strength,   durability,                and   surgical   \n",
       "3  experience,   Mersilene          mesh            remains        our   \n",
       "4    mucoceles  associated          with  phaeohyphomycosis         of   \n",
       "\n",
       "               0 Unnamed: 11 Unnamed: 12 mentoplasty using      Mersilene  \\\n",
       "0            NaN         NaN         NaN         NaN   NaN            NaN   \n",
       "1            yet          to          be      found.   NaN            NaN   \n",
       "2  adaptability.         NaN         NaN         NaN   NaN            NaN   \n",
       "3       material          of      choice         for  chin  augmentation.   \n",
       "4            the   paranasal    sinuses.         NaN   NaN            NaN   \n",
       "\n",
       "  mesh.  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
